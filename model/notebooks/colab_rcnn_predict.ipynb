{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27549,"status":"ok","timestamp":1675175985594,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"52A1M1fO83Wt","outputId":"5d55b538-f652-4797-b4f1-abe62c9ca7f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11677,"status":"ok","timestamp":1675175997263,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"8J6toYW39AKc","outputId":"4dfb4402-b74f-4bdf-c63f-ec1947545ea6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting albumentations==0.4.6\n","  Downloading albumentations-0.4.6.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 KB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.6) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.6) (1.7.3)\n","Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.6) (0.4.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.6) (6.0)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.6) (4.6.0.66)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.9.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.4.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (3.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2022.10.10)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.9)\n","Building wheels for collected packages: albumentations\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65173 sha256=87652dd59eb21bcf3b9692da97965a1cfc8405c96334bb7564f47c349d1e0d1c\n","  Stored in directory: /root/.cache/pip/wheels/d2/e3/0b/99a239413035502833a7b07283894243fddf5ce3aa720ca8dd\n","Successfully built albumentations\n","Installing collected packages: albumentations\n","  Attempting uninstall: albumentations\n","    Found existing installation: albumentations 1.2.1\n","    Uninstalling albumentations-1.2.1:\n","      Successfully uninstalled albumentations-1.2.1\n","Successfully installed albumentations-0.4.6\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.8/dist-packages (2.0.6)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from pycocotools) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pycocotools) (1.21.6)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n","Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.13.1+cu116)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision) (4.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n"]}],"source":["!pip install albumentations==0.4.6\n","!pip install pycocotools\n","!pip install torchvision"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1675175997264,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"OCXPAevp9B8p"},"outputs":[],"source":["import sys\n","sys.path.insert(0,'/content/drive/MyDrive/Skola/py-AI/wildfire/test_obj')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74723,"status":"ok","timestamp":1675176071973,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"7_iGe4yv9Ezo","outputId":"f67c9cff-f5a9-434f-87c8-f629e5d1518b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'vision'...\n","remote: Enumerating objects: 289636, done.\u001b[K\n","remote: Counting objects: 100% (10762/10762), done.\u001b[K\n","remote: Compressing objects: 100% (708/708), done.\u001b[K\n","remote: Total 289636 (delta 10110), reused 10627 (delta 10030), pack-reused 278874\n","Receiving objects: 100% (289636/289636), 586.92 MiB | 16.99 MiB/s, done.\n","Resolving deltas: 100% (265217/265217), done.\n","Note: switching to 'v0.8.2'.\n","\n","You are in 'detached HEAD' state. You can look around, make experimental\n","changes and commit them, and you can discard any commits you make in this\n","state without impacting any branches by switching back to a branch.\n","\n","If you want to create a new branch to retain commits you create, you may\n","do so (now or later) by using -c with the switch command. Example:\n","\n","  git switch -c <new-branch-name>\n","\n","Or undo this operation with:\n","\n","  git switch -\n","\n","Turn off this advice by setting config variable advice.detachedHead to false\n","\n","HEAD is now at 2f40a483d7 [v0.8.X] .circleci: Add Python 3.9 to CI (#3063)\n"]},{"data":{"text/plain":[]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","\n","git clone https://github.com/pytorch/vision.git\n","cd vision\n","git checkout v0.8.2\n","\n","cp references/detection/utils.py ../\n","cp references/detection/transforms.py ../\n","cp references/detection/coco_eval.py ../\n","cp references/detection/engine.py ../\n","cp references/detection/coco_utils.py ../"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8404,"status":"ok","timestamp":1675176080370,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"mDsZV9oH9I0r"},"outputs":[],"source":["import os\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torchvision.transforms as transform\n","import albumentations as A\n","import torchvision\n","import torch\n","import cv2\n","\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from albumentations.pytorch.transforms import ToTensorV2\n","from torchvision.models.detection import FasterRCNN\n","from engine import train_one_epoch, evaluate\n","from torch.utils.data import Dataset, DataLoader\n","from typing import Any"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1675176080372,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"PbMKhYwR9RNW","outputId":"9329bddc-5d19-4ba6-b698-02d83195a618"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-c8f5d6fe-483c-481b-b9c4-b973e08acd38\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>class</th>\n","      <th>xmin</th>\n","      <th>ymin</th>\n","      <th>xmax</th>\n","      <th>ymax</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ck0t4z1yrkmfv0794t2oqhd3f_jpeg.rf.17653d46c3ee...</td>\n","      <td>640</td>\n","      <td>480</td>\n","      <td>smoke</td>\n","      <td>514</td>\n","      <td>209</td>\n","      <td>616</td>\n","      <td>285</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ck0ujmglz85u10a468qt0d4fc_jpeg.rf.18d02755e8c9...</td>\n","      <td>640</td>\n","      <td>480</td>\n","      <td>smoke</td>\n","      <td>162</td>\n","      <td>210</td>\n","      <td>635</td>\n","      <td>302</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ck0uivtpc841h0a46ydgo7566_jpeg.rf.00134176dc29...</td>\n","      <td>640</td>\n","      <td>480</td>\n","      <td>smoke</td>\n","      <td>308</td>\n","      <td>206</td>\n","      <td>392</td>\n","      <td>252</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ck0nehpd69bax0721onacbe33_jpeg.rf.02ed50fbcb97...</td>\n","      <td>640</td>\n","      <td>480</td>\n","      <td>smoke</td>\n","      <td>274</td>\n","      <td>229</td>\n","      <td>454</td>\n","      <td>311</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ck0ow7vs07tuz08485n4yz5si_jpeg.rf.179fc0e59422...</td>\n","      <td>640</td>\n","      <td>480</td>\n","      <td>smoke</td>\n","      <td>386</td>\n","      <td>218</td>\n","      <td>607</td>\n","      <td>285</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8f5d6fe-483c-481b-b9c4-b973e08acd38')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c8f5d6fe-483c-481b-b9c4-b973e08acd38 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c8f5d6fe-483c-481b-b9c4-b973e08acd38');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                            filename  width  height  class  \\\n","0  ck0t4z1yrkmfv0794t2oqhd3f_jpeg.rf.17653d46c3ee...    640     480  smoke   \n","1  ck0ujmglz85u10a468qt0d4fc_jpeg.rf.18d02755e8c9...    640     480  smoke   \n","2  ck0uivtpc841h0a46ydgo7566_jpeg.rf.00134176dc29...    640     480  smoke   \n","3  ck0nehpd69bax0721onacbe33_jpeg.rf.02ed50fbcb97...    640     480  smoke   \n","4  ck0ow7vs07tuz08485n4yz5si_jpeg.rf.179fc0e59422...    640     480  smoke   \n","\n","   xmin  ymin  xmax  ymax  \n","0   514   209   616   285  \n","1   162   210   635   302  \n","2   308   206   392   252  \n","3   274   229   454   311  \n","4   386   218   607   285  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["test_df = pd.read_csv('/content/drive/MyDrive/Skola/py-AI/wildfire/test_obj/test_annotations.csv')\n","test_images = '/content/drive/MyDrive/Skola/py-AI/wildfire/test_obj/test'\n","\n","test_df.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1675176080944,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"7vhWMghW9Ovc"},"outputs":[],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","LABELS = test_df['class'].unique()\n","NUM_OF_CLASSES = len(LABELS)+1\n","\n","SAVE_PATH = '/content/drive/MyDrive/Skola/py-AI/wildfire/test_obj/models/'\n","MODEL_NAME = 'model_new.pt'\n","\n","SAMPLES = 16"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"executionInfo":{"elapsed":4082,"status":"ok","timestamp":1675176085016,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"1j-13i6IAZL3","outputId":"5c6cecfe-71ba-4238-b1f1-13e7ee2b71c0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c3d40d489a354779be10de151b8259be","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/97.8M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n","    pretrained=False,\n","    num_classes=NUM_OF_CLASSES,\n","    map_location=DEVICE\n","    )\n","IN_FEATURES = model.roi_heads.box_predictor.cls_score.in_features"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1675176085017,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"ggE5CBLl-LFQ"},"outputs":[],"source":["def collate_fn(batch: tuple) -> tuple:\n","  return tuple(zip(*batch))"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1675176085019,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"M638L6nYBBn6"},"outputs":[],"source":["def plot_predict_samples(loader, model, device, threshold) -> None:\n","  _images, _targets =next(iter(loader))\n","  images = [img.to(device) for img in _images]\n","  targets = [{k: v for k,v in target.items()} for target in _targets]\n","\n","  true_boxes = targets[0]['boxes'].cpu().numpy().astype(np.int32)\n","  sample = images[0].permute(1,2,0).cpu().numpy()\n","\n","  model.to(device)\n","  model.eval()\n","  cpu_device = torch.device('cpu')\n","\n","  _outputs = model(images)\n","  outputs = [{k: v.to(cpu_device) for k, v in target.items()} for target in _outputs]\n","\n","  fig, ax = plt.subplots(1, 1, figsize=(16,8))\n","\n","  for box in true_boxes:\n","    cv2.rectangle(\n","          img=sample,\n","          pt1=(box[0], box[1]),\n","          pt2=(box[2], box[3]),\n","          color=(1,0,0),\n","          thickness=1\n","      )\n","    \n","  pred_boxes = outputs[0]['boxes'].data.cpu().numpy().astype(np.int32)\n","  pred_scores = outputs[0]['scores'].data.cpu().numpy()\n","  pred_labels = outputs[0]['labels'].data.cpu().numpy().astype(np.int32)\n","\n","  box_counter = 0\n","  for box, score, label in zip(pred_boxes, pred_scores, pred_labels):\n","    if score > threshold:\n","      box_counter += 1\n","      cv2.rectangle(\n","          img=sample,\n","          pt1=(box[0], box[1]),\n","          pt2=(box[2], box[3]),\n","          color=(0,0,1),\n","          thickness=1\n","      )\n","\n","      cv2.putText(\n","          img=sample,\n","          text=f'{LABELS[label-1]} : {score: .2%}%',\n","          org=(box[0], box[1]-10),\n","          fontFace=cv2.FONT_HERSHEY_DUPLEX,\n","          fontScale=0.3,\n","          color=(0,0,0),\n","          thickness=1\n","      )\n","    print(f'Predicted {box_counter} (blue) : True {len(true_boxes)} (red)')\n","    ax.set_axis_off()\n","    ax.imshow((sample * 255).astype(np.uint8))\n","  "]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1675176085019,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"WC04n0Lj9mlS"},"outputs":[],"source":["class LabelMap:\n","  def __init__(self, labels: list) -> None:\n","    self._map = {c: i+1 for i, c in enumerate(labels)}\n","    self.reversed_map = {i: c for i, c in enumerate(labels)}\n","\n","  def fit(self, df: pd.DataFrame, col: str) -> pd.DataFrame:\n","    df[col] = df[col].map(self._map)\n","    return df\n","\n","\n","def encode_label(df: pd.DataFrame, col: str, map_dict: dict) -> pd.DataFrame:\n","  df[col] = df[col].map(map_dict)\n","  return df"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1675176085020,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"WdnpOPIV-AoL"},"outputs":[],"source":["class WildfireDataset(Dataset):\n","  def __init__(self, df: pd.DataFrame, img_path: str, labels: list, transforms: Any = None, **kwargs) -> None:\n","    super().__init__(**kwargs)\n","    self.df = df\n","    self.img_path = img_path\n","    self.labels = labels\n","    self.images = self.df['filename'].unique()\n","    self.transforms = transforms\n","\n","  def __len__(self) -> int:\n","    return len(self.images)\n","\n","  def __getitem__(self, i: int) -> tuple:\n","    img_file = os.path.join(self.img_path, self.images[i])\n","\n","    img = cv2.imread(img_file)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img = img.astype(np.float32)\n","    img = img/255.0\n","\n","    img_data = self.df.loc[self.df['filename'] == self.images[i]]\n","\n","    xmins = img_data['xmin'].values\n","    ymins = img_data['ymin'].values\n","    xmaxs = img_data['xmax'].values\n","    ymaxs = img_data['ymax'].values\n","\n","    boxes = torch.as_tensor(np.stack([xmins, ymins, xmaxs, ymaxs], axis=1), dtype=torch.float32)\n","    labels = torch.as_tensor(img_data['class'].values, dtype=torch.int64)\n","    _id = torch.tensor([i])\n","\n","    areas = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0])\n","    areas = torch.as_tensor(areas, dtype=torch.float32)\n","\n","    iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n","\n","    target = dict()\n","    target['boxes'] = boxes\n","    target['labels'] = labels\n","    target['image_id'] = _id\n","    target['area'] = areas\n","    target['iscrowd'] = iscrowd\n","\n","    if self.transforms:\n","      transformed = self.transforms(image=img, bboxes=boxes, labels=labels)\n","      img = transformed['image']\n","      target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)\n","    \n","    return torch.as_tensor(img, dtype=torch.float32), target\n","\n","  def get_h_w(self, image: str) -> tuple:\n","    \"\"\"Get height and width of image\"\"\"\n","    img_data = self.df.loc[self.df['filename'] == image]\n","    return img_data['width'].values[0], img_data['height'].values[0]"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20177,"status":"ok","timestamp":1675176105186,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"M0tXKM49-qcq","outputId":"afce65c2-808f-49a8-a1ad-3fdb7d1f47a5"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["model.load_state_dict(torch.load(SAVE_PATH + MODEL_NAME))"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1675176105188,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"RvkY90be9wVa"},"outputs":[],"source":["label_map = LabelMap(LABELS)\n","test_df = encode_label(test_df, 'class', label_map._map)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1675176105189,"user":{"displayName":"Andreas Eliasson","userId":"14675535218151679092"},"user_tz":-60},"id":"lsEZBzi0AhpJ"},"outputs":[],"source":["test_transform = A.Compose(\n","    [ToTensorV2(p=1)],\n","    bbox_params=A.BboxParams(format='pascal voc', label_fields=['labels'])\n",")\n","\n","test_dataset = WildfireDataset(test_df, test_images, LABELS, test_transform)\n","test_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=4,\n","    shuffle=True,\n","    num_workers=2,\n","    collate_fn=collate_fn\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ypjBWiAGTIJNCHM3kwDskwiMjwryiyy1"},"id":"MKlGPkEQBFa3","outputId":"a582fc03-c647-4ddf-8de7-181ec1f41e5d"},"outputs":[],"source":["for i in range(SAMPLES):\n","  plot_predict_samples(test_dataloader, model, DEVICE, 0.6)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"06xn8jVGLMbr"},"outputs":[],"source":["from PIL import Image\n","\n","def get_relevant_scores(threshold: float, boxes: list, scores: list, labels: list) -> tuple:\n","    \"\"\"\n","    Get score over a set lower accuracy limit\n","    :param labels: list; predicted labels\n","    :param scores: list; predicted accuracy scores\n","    :param boxes: list; predicted bounding boxes\n","    :return: tuple[list, list, list];\n","    \"\"\"\n","    x = len([score for score in scores if score >= threshold])\n","    return boxes[:x], scores[:x], labels[:x]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RruCooxeVQ9L"},"outputs":[{"ename":"SyntaxError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-122874294022>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    ...     boxes=torch.tensor([[258.0, 41.0, 606.0, 285.0]]),\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","preds = [\n","   dict(\n","...     boxes=torch.tensor([[258.0, 41.0, 606.0, 285.0]]),\n","...     scores=torch.tensor([0.536]),\n","...     labels=torch.tensor([0]),\n","...   )\n","... ]\n",">>> target = [\n","...   dict(\n","...     boxes=torch.tensor([[214.0, 41.0, 562.0, 285.0]]),\n","...     labels=torch.tensor([0]),\n","...   )\n","... ]\n","metric = MeanAveragePrecision()\n","metric.update(preds, target)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5tb3KbdqK-2p"},"outputs":[],"source":["metric.compute()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dnl2VioJLbg2"},"outputs":[],"source":["targets = list()\n","\n","for image in test_dataset:\n","  targets.append(dict(boxes=image[1]['boxes'], labels=image[1]['labels']))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eArO8jMxLcn3"},"outputs":[],"source":["target = [\n","   dict(\n","     boxes=torch.tensor([[214.0, 41.0, 562.0, 285.0]]),\n","     labels=torch.tensor([0]),\n","   )\n","]\n","target"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hBuBScyBOl9u"},"outputs":[],"source":["for image in os.listdir(test_images):\n","  img_path = test_images + '/' + image\n","  predictions = predict(model, img_path, 0.0)\n","  break\n","predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lIeTNWLcPIVs"},"outputs":[],"source":["import torchvision.transforms as transforms\n","\n","def predict(model, img_path: str, threshold: float) -> tuple:\n","    transform = transforms.Compose([transforms.ToTensor()])\n","    img = Image.open(img_path)\n","    pred_img = transform(img)\n","    pred_img = pred_img.view(1, 3, pred_img.shape[1], pred_img.shape[2])\n","\n","    preds = model(pred_img)\n","    outputs = [{k: v.to(torch.device('cpu')) for k, v in target.items()} for target in preds]\n","\n","    boxes = outputs[0]['boxes'].data.cpu().numpy().astype(np.int32)\n","    scores = outputs[0]['scores'].data.cpu().numpy()\n","    labels = outputs[0]['labels'].data.cpu().numpy().astype(np.int32)\n","\n","    boxes, scores, labels = get_relevant_scores(threshold, boxes, scores, labels)\n","    \n","    return boxes, scores, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"If1gn2nwPpGA"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMhslw9db8Kbop6lIn9OxNL","name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"venv310","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.5"},"vscode":{"interpreter":{"hash":"bf12dd39f5f7ed06ba50c0dbf677809c828302383f5904e167102ce889f09ad7"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"4e0b57e96c7d4ffb95107b31a71a4883":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2f53735d5d94349821dad4aa4627631","placeholder":"​","style":"IPY_MODEL_b2c69ccf2cba4d25a20b525c42ff64d4","value":"100%"}},"61022baf600c4771b220e7b89c8acd64":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"630c29884d444b61ba2ec7230fec330d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7202ca84760e47aa9c448aecc2906d00":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72acd65906964a128babcce5f13dd091":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7202ca84760e47aa9c448aecc2906d00","max":102530333,"min":0,"orientation":"horizontal","style":"IPY_MODEL_630c29884d444b61ba2ec7230fec330d","value":102530333}},"b2c69ccf2cba4d25a20b525c42ff64d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2f53735d5d94349821dad4aa4627631":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b34d60b1a5f74eb2b075b283017b2df8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c033ad62e8e3482ea0ebcc4fd10dff18":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61022baf600c4771b220e7b89c8acd64","placeholder":"​","style":"IPY_MODEL_b34d60b1a5f74eb2b075b283017b2df8","value":" 97.8M/97.8M [00:02&lt;00:00, 41.3MB/s]"}},"c3d40d489a354779be10de151b8259be":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4e0b57e96c7d4ffb95107b31a71a4883","IPY_MODEL_72acd65906964a128babcce5f13dd091","IPY_MODEL_c033ad62e8e3482ea0ebcc4fd10dff18"],"layout":"IPY_MODEL_c4f382e9d49f4544a11028e66bb79b64"}},"c4f382e9d49f4544a11028e66bb79b64":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
