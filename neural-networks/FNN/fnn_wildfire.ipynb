{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "bLVgH3baaBPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4SWvO5-ZlEb"
      },
      "outputs": [],
      "source": [
        "!sudo apt install python3.9\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1\n",
        "!sudo update-alternatives --config python3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "sdD3N_60ac_e"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "8N-1DCqjczoQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters \n",
        "input_size = 62500 # 250x250 RGB inage size\n",
        "num_classes = 1\n",
        "neurons_per_layer=250\n",
        "epochs = 5\n",
        "batch_size = 25\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "w4NakH1FdXeh"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FNN(nn.Module):\n",
        "    def __init__(self, n_inputs, n_neurons, num_classes):\n",
        "        super(FNN, self).__init__()\n",
        "\n",
        "        self.activation = nn.Sigmoid()\n",
        "        self.linear_layer_stack = nn.Sequential(\n",
        "            # input layer\n",
        "            nn.Linear(n_inputs, n_neurons),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # hidden layer\n",
        "            nn.Linear(n_neurons, n_neurons),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_neurons, n_neurons),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_neurons, n_neurons),\n",
        "\n",
        "            # output layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_neurons, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "      output = self.linear_layer_stack(input)\n",
        "      return self.activation(output)\n",
        "\n",
        "\n",
        "model = FNN(input_size, neurons_per_layer, num_classes)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "Sw7TswPUddx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795f11b1-a55e-49e9-8ee6-83a9c7927a72"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FNN(\n",
              "  (activation): Sigmoid()\n",
              "  (linear_layer_stack): Sequential(\n",
              "    (0): Linear(in_features=62500, out_features=250, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=250, out_features=250, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=250, out_features=250, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=250, out_features=250, bias=True)\n",
              "    (7): ReLU()\n",
              "    (8): Linear(in_features=250, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "WTmO0Ai9dpim"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WildFireDataset(Dataset):\n",
        "  train_mode = True\n",
        "\n",
        "  def __init__(self, custom_dataset, transforms=None, shuffle=False, test_size=0):\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.train_dataset = self.process_dataset(custom_dataset)\n",
        "\n",
        "    self.transforms = transforms() if transforms is not None else None\n",
        "\n",
        "    if shuffle:\n",
        "      random.shuffle(self.train_dataset)\n",
        "\n",
        "    if test_size > 0:\n",
        "      if test_size > 1:\n",
        "        raise ValueError(\"Error: Test must be between 0-1.\\nRepresenting 0% to 100%\")\n",
        "      test_len = int(len(self.train_dataset) * test_size)\n",
        "      self.test_dataset = self.train_dataset[:test_len]\n",
        "      self.train_dataset = self.train_dataset[test_len:]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.train_dataset) if self.train_mode else len(self.test_dataset)\n",
        "\n",
        "  def process_dataset(self, dataset):\n",
        "    custom_dataset = []\n",
        "    for data in dataset:\n",
        "      image = data[0]\n",
        "      image = self.flatten(image)\n",
        "      image = image[0]\n",
        "      image = image.to(torch.float)\n",
        "      label = torch.tensor(data[1], dtype=torch.float)\n",
        "      custom_dataset.append((image, label))\n",
        "    return custom_dataset\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    image, label = self.train_dataset[index] if self.train_mode else self.test_dataset[index]\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "A8LtDvZbwm-T"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path, label, size=500):\n",
        "  dataset = []\n",
        "  for img in [path+file for file in os.listdir(path)][:size]:\n",
        "    image = read_image(img)\n",
        "    dataset.append((image, [label]))\n",
        "  return dataset\n",
        "\n",
        "PATH_FIRE = \"/content/drive/MyDrive/wildfire-dataset/fire/\"\n",
        "PATH_NO_FIRE = \"/content/drive/MyDrive/wildfire-dataset/no-fire/\"\n",
        "\n",
        "DATASET = []\n",
        "DATASET += load_dataset(PATH_FIRE, 1.0)\n",
        "DATASET += load_dataset(PATH_NO_FIRE, 0.0)\n",
        "\n",
        "wildfire_dataset = WildFireDataset(DATASET, shuffle=True, test_size=0.2)"
      ],
      "metadata": {
        "id": "jUNAWTA__9ks"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, dataset):\n",
        "  # Train the model\n",
        "  n_total_steps = len(dataset)\n",
        "  for epoch in range(epochs):\n",
        "      for i, (image, labels) in enumerate(dataset):\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(image)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          if (i+1) % 100 == 0:\n",
        "              print (f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "tnRqtcmgdvbI"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*20, \"Training\", \"=\"*20)\n",
        "wildfire_dataset.train_mode = True\n",
        "print(\"Dataset size:\", len(wildfire_dataset))\n",
        "ok = input(\"Train [y/n]: \").lower()\n",
        "if ok == \"y\" or ok == \"yes\":\n",
        "  print(\"Begin...\")\n",
        "  train(epochs, wildfire_dataset)\n",
        "else:\n",
        "  print(\"Aborted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ3akvW_4WEZ",
        "outputId": "66e159ca-c507-423b-e8d4-22af2d989958"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================== Training ====================\n",
            "Dataset size: 800\n",
            "Train [y/n]: y\n",
            "Begin...\n",
            "Epoch [1/5], Step [100/800], Loss: 100.0000\n",
            "Epoch [1/5], Step [200/800], Loss: 100.0000\n",
            "Epoch [1/5], Step [300/800], Loss: 0.0000\n",
            "Epoch [1/5], Step [400/800], Loss: 100.0000\n",
            "Epoch [1/5], Step [500/800], Loss: 0.0000\n",
            "Epoch [1/5], Step [600/800], Loss: 0.0000\n",
            "Epoch [1/5], Step [700/800], Loss: 0.0000\n",
            "Epoch [1/5], Step [800/800], Loss: 0.0000\n",
            "Epoch [2/5], Step [100/800], Loss: 100.0000\n",
            "Epoch [2/5], Step [200/800], Loss: 100.0000\n",
            "Epoch [2/5], Step [300/800], Loss: 0.0000\n",
            "Epoch [2/5], Step [400/800], Loss: 100.0000\n",
            "Epoch [2/5], Step [500/800], Loss: 0.0000\n",
            "Epoch [2/5], Step [600/800], Loss: 0.0000\n",
            "Epoch [2/5], Step [700/800], Loss: 0.0000\n",
            "Epoch [2/5], Step [800/800], Loss: 0.0000\n",
            "Epoch [3/5], Step [100/800], Loss: 100.0000\n",
            "Epoch [3/5], Step [200/800], Loss: 100.0000\n",
            "Epoch [3/5], Step [300/800], Loss: 0.0000\n",
            "Epoch [3/5], Step [400/800], Loss: 100.0000\n",
            "Epoch [3/5], Step [500/800], Loss: 0.0000\n",
            "Epoch [3/5], Step [600/800], Loss: 0.0000\n",
            "Epoch [3/5], Step [700/800], Loss: 0.0000\n",
            "Epoch [3/5], Step [800/800], Loss: 0.0000\n",
            "Epoch [4/5], Step [100/800], Loss: 100.0000\n",
            "Epoch [4/5], Step [200/800], Loss: 100.0000\n",
            "Epoch [4/5], Step [300/800], Loss: 0.0000\n",
            "Epoch [4/5], Step [400/800], Loss: 100.0000\n",
            "Epoch [4/5], Step [500/800], Loss: 0.0000\n",
            "Epoch [4/5], Step [600/800], Loss: 0.0000\n",
            "Epoch [4/5], Step [700/800], Loss: 0.0000\n",
            "Epoch [4/5], Step [800/800], Loss: 0.0000\n",
            "Epoch [5/5], Step [100/800], Loss: 100.0000\n",
            "Epoch [5/5], Step [200/800], Loss: 100.0000\n",
            "Epoch [5/5], Step [300/800], Loss: 0.0000\n",
            "Epoch [5/5], Step [400/800], Loss: 100.0000\n",
            "Epoch [5/5], Step [500/800], Loss: 0.0000\n",
            "Epoch [5/5], Step [600/800], Loss: 0.0000\n",
            "Epoch [5/5], Step [700/800], Loss: 0.0000\n",
            "Epoch [5/5], Step [800/800], Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(epochs, test_dataset):\n",
        "  # Test the model\n",
        "  # In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "  with torch.no_grad():\n",
        "      n_correct = 0\n",
        "      n_samples = 0\n",
        "      for images, labels in test_dataset:\n",
        "          labels = labels.to(device)\n",
        "          prediction = model(images).data\n",
        "          n_samples += labels.size(0)\n",
        "          n_correct += (prediction == labels)\n",
        "\n",
        "      acc = 100.0 * (n_correct / n_samples)\n",
        "      print(f'Accuracy of {len(test_dataset)} test images: {acc} %')"
      ],
      "metadata": {
        "id": "58Qs-FCrdFF0"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*20, \"Testing\", \"=\"*20)\n",
        "wildfire_dataset.train_mode = False\n",
        "print(\"Dataset size:\", len(wildfire_dataset))\n",
        "ok = input(\"Test [y/n]: \").lower()\n",
        "if ok == \"y\" or ok == \"yes\":\n",
        "  print(\"Begin...\")\n",
        "  predict(epochs, wildfire_dataset)\n",
        "else:\n",
        "  print(\"Aborted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC_iC0xAIjWV",
        "outputId": "f30ff1a7-3615-4d76-9639-29c4fc44a1d8"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================== Testing ====================\n",
            "Dataset size: 200\n",
            "Test [y/n]: y\n",
            "Begin...\n",
            "Accuracy of 200 test images: tensor([52.]) %\n"
          ]
        }
      ]
    }
  ]
}