{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b777298d63f04b419d7c13e72a77a0eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cd5dd6ba7e94d7a908a4eff88f77ad7",
              "IPY_MODEL_c0a796ffe2484c2393934610c0270e1b",
              "IPY_MODEL_a1b61ff0bc4e48909abd6289ccd2190f"
            ],
            "layout": "IPY_MODEL_1dc25fde1dec4f088a27cdeeec448ffd"
          }
        },
        "6cd5dd6ba7e94d7a908a4eff88f77ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39c321010773431ab64777b846a8d582",
            "placeholder": "​",
            "style": "IPY_MODEL_6b0b21a456f84edd943aef2c2ad8bd3e",
            "value": "100%"
          }
        },
        "c0a796ffe2484c2393934610c0270e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40d5dac3f88348da975803868e8f562c",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fa12e0a2a364350a819c9deb949d11a",
            "value": 102530333
          }
        },
        "a1b61ff0bc4e48909abd6289ccd2190f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b843593c3e343c9a61f2929468bf3f2",
            "placeholder": "​",
            "style": "IPY_MODEL_6eeb65498b2341f18fc924cb491756bc",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 236MB/s]"
          }
        },
        "1dc25fde1dec4f088a27cdeeec448ffd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39c321010773431ab64777b846a8d582": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b0b21a456f84edd943aef2c2ad8bd3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40d5dac3f88348da975803868e8f562c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fa12e0a2a364350a819c9deb949d11a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b843593c3e343c9a61f2929468bf3f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eeb65498b2341f18fc924cb491756bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antonsen2/wildfire-ai/blob/research/obj_det_train_ray_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FYbIgO7K8uEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb81865a-48ef-481f-bb82-711ccf5381c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations==0.4.6\n",
        "!pip install pycocotools\n",
        "!pip install torchvision\n",
        "!pip install ray"
      ],
      "metadata": {
        "id": "7hwbXulM8_qa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4913ffc-32f0-41c2-bef8-d539f9fe37c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting albumentations==0.4.6\n",
            "  Downloading albumentations-0.4.6.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.6) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.6) (1.7.3)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.6) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.6) (6.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==0.4.6) (4.6.0.66)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.0.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.9.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2022.10.10)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.9)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65173 sha256=d630cb0f619757e9b3eae9dce3a47a51e498c897a952c3be5d118f0569250ff8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/e3/0b/99a239413035502833a7b07283894243fddf5ce3aa720ca8dd\n",
            "Successfully built albumentations\n",
            "Installing collected packages: albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.2.1\n",
            "    Uninstalling albumentations-1.2.1:\n",
            "      Successfully uninstalled albumentations-1.2.1\n",
            "Successfully installed albumentations-0.4.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.8/dist-packages (2.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pycocotools) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from pycocotools) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision) (4.4.0)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ray\n",
            "  Downloading ray-2.2.0-cp38-cp38-manylinux2014_x86_64.whl (57.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from ray) (1.21.6)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.8/dist-packages (from ray) (1.3.3)\n",
            "Collecting virtualenv>=20.0.24\n",
            "  Downloading virtualenv-20.17.1-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from ray) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from ray) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from ray) (3.9.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.8/dist-packages (from ray) (3.19.6)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from ray) (4.3.3)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.8/dist-packages (from ray) (1.3.1)\n",
            "Requirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from ray) (1.51.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ray) (1.0.4)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from ray) (22.2.0)\n",
            "Collecting distlib<1,>=0.3.6\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 KB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<3,>=2.4 in /usr/local/lib/python3.8/dist-packages (from virtualenv>=20.0.24->ray) (2.6.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray) (5.10.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray) (0.19.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->ray) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->ray) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray) (3.11.0)\n",
            "Installing collected packages: distlib, virtualenv, ray\n",
            "Successfully installed distlib-0.3.6 ray-2.2.0 virtualenv-20.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive/Skola/py-AI/wildfire/test_obj')"
      ],
      "metadata": {
        "id": "owpkJRD89CtF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.8.2\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "metadata": {
        "id": "6xGS2f-T9Fp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "918f622b-cfd7-471a-acc4-7caaae7ff00c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 284999, done.\u001b[K\n",
            "remote: Counting objects: 100% (6125/6125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (374/374), done.\u001b[K\n",
            "remote: Total 284999 (delta 5798), reused 6036 (delta 5738), pack-reused 278874\u001b[K\n",
            "Receiving objects: 100% (284999/284999), 571.53 MiB | 16.47 MiB/s, done.\n",
            "Resolving deltas: 100% (260905/260905), done.\n",
            "Note: switching to 'v0.8.2'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 2f40a483d7 [v0.8.X] .circleci: Add Python 3.9 to CI (#3063)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transform\n",
        "import albumentations as A\n",
        "import torchvision\n",
        "import torch\n",
        "import cv2\n",
        "\n",
        "from functools import partial\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from engine import train_one_epoch, evaluate\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import Any\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ],
      "metadata": {
        "id": "LSUVFqBr9Jv5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)"
      ],
      "metadata": {
        "id": "K0_JASxnASq6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "b777298d63f04b419d7c13e72a77a0eb",
            "6cd5dd6ba7e94d7a908a4eff88f77ad7",
            "c0a796ffe2484c2393934610c0270e1b",
            "a1b61ff0bc4e48909abd6289ccd2190f",
            "1dc25fde1dec4f088a27cdeeec448ffd",
            "39c321010773431ab64777b846a8d582",
            "6b0b21a456f84edd943aef2c2ad8bd3e",
            "40d5dac3f88348da975803868e8f562c",
            "6fa12e0a2a364350a819c9deb949d11a",
            "3b843593c3e343c9a61f2929468bf3f2",
            "6eeb65498b2341f18fc924cb491756bc"
          ]
        },
        "outputId": "debd0f16-46ba-4e2e-985d-1e9420596ff1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b777298d63f04b419d7c13e72a77a0eb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/Skola/py-AI/wildfire/test_obj/train_annotations.csv')\n",
        "valid_df = pd.read_csv('/content/drive/MyDrive/Skola/py-AI/wildfire/test_obj/valid_annotations.csv')\n",
        "\n",
        "train_images = '/content/drive/MyDrive/Skola/py-AI/wildfire/test_obj/train'\n",
        "valid_images = '/content/drive/MyDrive/Skola/py-AI/wildfire/test_obj/valid'\n",
        "\n",
        "train_df.tail()"
      ],
      "metadata": {
        "id": "uly4JwRm9SFg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "9c5c8cd9-e1a5-48e1-e13d-1496b1f58a2a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        filename  width  height class  xmin  ymin  xmax  ymax\n",
              "1200  fire-9.jpg    640     480  fire    65   306   208   355\n",
              "1201  fire-9.jpg    640     480  fire     1   350    89   377\n",
              "1202  fire-9.jpg    640     480  fire   496   363   527   386\n",
              "1203  fire-9.jpg    640     480  fire   459   342   486   371\n",
              "1204  fire-9.jpg    640     480  fire   612   332   640   353"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ef56c7fc-c282-4c1a-81be-8567114aa067\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>class</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1200</th>\n",
              "      <td>fire-9.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>fire</td>\n",
              "      <td>65</td>\n",
              "      <td>306</td>\n",
              "      <td>208</td>\n",
              "      <td>355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1201</th>\n",
              "      <td>fire-9.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>fire</td>\n",
              "      <td>1</td>\n",
              "      <td>350</td>\n",
              "      <td>89</td>\n",
              "      <td>377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1202</th>\n",
              "      <td>fire-9.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>fire</td>\n",
              "      <td>496</td>\n",
              "      <td>363</td>\n",
              "      <td>527</td>\n",
              "      <td>386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1203</th>\n",
              "      <td>fire-9.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>fire</td>\n",
              "      <td>459</td>\n",
              "      <td>342</td>\n",
              "      <td>486</td>\n",
              "      <td>371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1204</th>\n",
              "      <td>fire-9.jpg</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>fire</td>\n",
              "      <td>612</td>\n",
              "      <td>332</td>\n",
              "      <td>640</td>\n",
              "      <td>353</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef56c7fc-c282-4c1a-81be-8567114aa067')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ef56c7fc-c282-4c1a-81be-8567114aa067 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ef56c7fc-c282-4c1a-81be-8567114aa067');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPU = True\n",
        "DEVICE = torch.device('cuda') if GPU else torch.device('cpu')\n",
        "\n",
        "LABELS = train_df['class'].unique()\n",
        "NUM_OF_CLASSES = len(LABELS)+1\n",
        "IN_FEATURES = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "SAVE_PATH = '/content/drive/MyDrive/Skola/py-AI/wildfire/test_obj/models/'\n",
        "MODEL_NAME = 'model_include_fire.pt'"
      ],
      "metadata": {
        "id": "N5WCubPG9PVw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch: tuple) -> tuple:\n",
        "  return tuple(zip(*batch))\n",
        "\n",
        "def train(config, model, train_loader, valid_loader) -> None:\n",
        "  params = [param for param in model.parameters() if param.requires_grad]\n",
        "  #optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)  # TODO: Optimize params\n",
        "  optimizer = torch.optim.SGD(params, lr=config[\"lr\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "  #lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)  # TODO: Optimize params\n",
        "  lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=config[\"step_size\"], gamma=config[\"gamma\"])\n",
        "\n",
        "  for epoch in range(config[\"epoch\"]):\n",
        "    train_one_epoch(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        data_loader=train_loader,\n",
        "        device=DEVICE,\n",
        "        epoch=epoch,\n",
        "        print_freq=len(train_loader)\n",
        "    )\n",
        "    \n",
        "    lr_schedule.step()\n",
        "    evaluate(\n",
        "        model=model,\n",
        "        data_loader=valid_loader,\n",
        "        device=DEVICE\n",
        "    )"
      ],
      "metadata": {
        "id": "LcpzPv-o-McD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(images, targets) -> None:\n",
        "  max_images = 4\n",
        "  img_counter = 0\n",
        "\n",
        "  for image, target in zip(images, targets):\n",
        "    if img_counter == max_images:\n",
        "      break\n",
        "\n",
        "    img_counter += 1\n",
        "\n",
        "    sample = image.permute(1,2,0).cpu().numpy()\n",
        "    boxes = target['boxes'].cpu().numpy().astype(np.int32)\n",
        "    labels = target['labels'].cpu().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10,8))\n",
        "\n",
        "    for i, box in enumerate(boxes):\n",
        "      cv2.rectangle(\n",
        "          img=sample,\n",
        "          pt1=(box[0], box[1]),\n",
        "          pt2=(box[2], box[3]),\n",
        "          color=(1,0,0),\n",
        "          thickness=1\n",
        "      )\n",
        "      cv2.putText(\n",
        "          img=sample,\n",
        "          text=labels[i],\n",
        "          org=(box[0], box[1]-10),\n",
        "          fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
        "          fontScale=0.3,\n",
        "          color=(0,0,0),\n",
        "          thickness=1\n",
        "      )\n",
        "    \n",
        "    ax.set_axis_off();\n",
        "    ax.imshow((sample * 255).astype(np.uint8))"
      ],
      "metadata": {
        "id": "TcKFoX7O_Vi6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelMap:\n",
        "  def __init__(self, labels: list) -> None:\n",
        "    self._map = {c: i+1 for i, c in enumerate(labels)}\n",
        "    self.reversed_map = {i: c for i, c in enumerate(labels)}\n",
        "\n",
        "  def fit(self, df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
        "    df[col] = df[col].map(self._map)\n",
        "    return df"
      ],
      "metadata": {
        "id": "XI8mXToz9oaZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WildfireDataset(Dataset):\n",
        "  def __init__(self, df: pd.DataFrame, img_path: str, labels: list, transforms: Any = None, **kwargs) -> None:\n",
        "    super().__init__(**kwargs)\n",
        "    self.df = df\n",
        "    self.img_path = img_path\n",
        "    self.labels = labels\n",
        "    self.images = self.df['filename'].unique()\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, i: int) -> tuple:\n",
        "    img_file = os.path.join(self.img_path, self.images[i])\n",
        "\n",
        "    img = cv2.imread(img_file)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img.astype(np.float32)\n",
        "    img = img/255.0\n",
        "\n",
        "    img_data = self.df.loc[self.df['filename'] == self.images[i]]\n",
        "\n",
        "    xmins = img_data['xmin'].values\n",
        "    ymins = img_data['ymin'].values\n",
        "    xmaxs = img_data['xmax'].values\n",
        "    ymaxs = img_data['ymax'].values\n",
        "\n",
        "    boxes = torch.as_tensor(np.stack([xmins, ymins, xmaxs, ymaxs], axis=1), dtype=torch.float32)\n",
        "    labels = torch.as_tensor(img_data['class'].values, dtype=torch.int64)\n",
        "    _id = torch.tensor([i])\n",
        "\n",
        "    areas = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0])\n",
        "    areas = torch.as_tensor(areas, dtype=torch.float32)\n",
        "\n",
        "    iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n",
        "\n",
        "    target = dict()\n",
        "    target['boxes'] = boxes\n",
        "    target['labels'] = labels\n",
        "    target['image_id'] = _id\n",
        "    target['area'] = areas\n",
        "    target['iscrowd'] = iscrowd\n",
        "\n",
        "    if self.transforms:\n",
        "      transformed = self.transforms(image=img, bboxes=boxes, labels=labels)\n",
        "      img = transformed['image']\n",
        "      target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)\n",
        "    \n",
        "    return torch.as_tensor(img, dtype=torch.float32), target\n",
        "\n",
        "  def get_h_w(self, image: str) -> tuple:\n",
        "    \"\"\"Get height and width of image\"\"\"\n",
        "    img_data = self.df.loc[self.df['filename'] == image]\n",
        "    return img_data['width'].values[0], img_data['height'].values[0]"
      ],
      "metadata": {
        "id": "A8_nQhxz-BbZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = LabelMap(LABELS)  # 1='smoke', 2='fire'\n",
        "\n",
        "train_df = label_map.fit(train_df, 'class')\n",
        "valid_df = label_map.fit(valid_df, 'class')\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "wJQ1UFgM9scA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "01dcfb06-33be-433e-ae46-e7af7ca029e3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            filename  width  height  class  \\\n",
              "0  ck0qd8gs6ko7j0721x25cv4o3_jpeg.rf.005f5707706e...    640     480      1   \n",
              "1  ck0t40rhdz68s0a46ekx049a6_jpeg.rf.00403179fe5f...    640     480      1   \n",
              "2  ck0m0ch9ugnna07940o8x989j_jpeg.rf.0101cdb46a16...    640     480      1   \n",
              "3  ck0rr6bfa9b3w0721aw5unwdy_jpeg.rf.00982c053d66...    640     480      1   \n",
              "4  ck0uk75x5ysls0721e5a9j891_jpeg.rf.00d7fd8503e1...    640     480      1   \n",
              "\n",
              "   xmin  ymin  xmax  ymax  \n",
              "0   125   190   177   286  \n",
              "1   326   207   494   249  \n",
              "2   308   166   582   257  \n",
              "3   241   204   310   244  \n",
              "4   523   208   619   288  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-27be6960-7627-45f0-ab9f-1b3e7c24eecd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>class</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ck0qd8gs6ko7j0721x25cv4o3_jpeg.rf.005f5707706e...</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>1</td>\n",
              "      <td>125</td>\n",
              "      <td>190</td>\n",
              "      <td>177</td>\n",
              "      <td>286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ck0t40rhdz68s0a46ekx049a6_jpeg.rf.00403179fe5f...</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>1</td>\n",
              "      <td>326</td>\n",
              "      <td>207</td>\n",
              "      <td>494</td>\n",
              "      <td>249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ck0m0ch9ugnna07940o8x989j_jpeg.rf.0101cdb46a16...</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>1</td>\n",
              "      <td>308</td>\n",
              "      <td>166</td>\n",
              "      <td>582</td>\n",
              "      <td>257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ck0rr6bfa9b3w0721aw5unwdy_jpeg.rf.00982c053d66...</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>1</td>\n",
              "      <td>241</td>\n",
              "      <td>204</td>\n",
              "      <td>310</td>\n",
              "      <td>244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ck0uk75x5ysls0721e5a9j891_jpeg.rf.00d7fd8503e1...</td>\n",
              "      <td>640</td>\n",
              "      <td>480</td>\n",
              "      <td>1</td>\n",
              "      <td>523</td>\n",
              "      <td>208</td>\n",
              "      <td>619</td>\n",
              "      <td>288</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27be6960-7627-45f0-ab9f-1b3e7c24eecd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-27be6960-7627-45f0-ab9f-1b3e7c24eecd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-27be6960-7627-45f0-ab9f-1b3e7c24eecd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Optimize params\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "      A.HorizontalFlip(p=0.5),\n",
        "      A.RandomBrightnessContrast(p=0.2),\n",
        "      ToTensorV2(p=1)\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])\n",
        ")\n",
        "valid_transform = A.Compose(\n",
        "    [ToTensorV2(p=1)],\n",
        "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])\n",
        ")"
      ],
      "metadata": {
        "id": "LRwRM8sg-HHz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = WildfireDataset(train_df, train_images, LABELS, train_transform)\n",
        "valid_dataset = WildfireDataset(valid_df, valid_images, LABELS, valid_transform)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "QvoNf4Me-X6J"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# images, targets = next(iter(train_dataloader))\n",
        "# plot_images(images, targets)"
      ],
      "metadata": {
        "id": "g9so1AUH-a_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(IN_FEATURES, NUM_OF_CLASSES)\n",
        "model = model.to(DEVICE) "
      ],
      "metadata": {
        "id": "n4sZkS0P-n5Y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.device(\"cuda\"))\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj3BZxm8X3pj",
        "outputId": "bc080488-3cf6-4011-ed67-c562b7588af7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "search_space = {\n",
        "    \"lr\": tune.choice([0.005, 0.010]),\n",
        "    \"momentum\": tune.choice([0.9, 1.8]),\n",
        "    \"weight_decay\": tune.choice([0.0005, 0.0010]),\n",
        "    \"step_size\": tune.choice([3, 6]),\n",
        "    \"gamma\": tune.choice([0.1, 0.2]),\n",
        "    \"epoch\": tune.choice([10, 20])\n",
        "}\n",
        "\n",
        "scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=1,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "\n",
        "reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "\n",
        "result = tune.run(\n",
        "  # Will throw ValueError: The actor ImplicitFunc is too large (158 MiB > FUNCTION_SIZE_ERROR_THRESHOLD=95 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
        "  partial(train, model, train_dataloader, valid_dataloader),\n",
        "  # Which can be fixed with tune_with_parameters - see link below \n",
        "  # https://docs.ray.io/en/master/tune/api_docs/trainable.html#tune-with-parameters\n",
        "  # But this will instead throw RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
        "  # tune.with_parameters(train, model=model, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader),\n",
        "  config=search_space,\n",
        "  num_samples=1,\n",
        "  scheduler=scheduler,\n",
        "  progress_reporter=reporter)\n",
        "\n",
        "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "print(\"Best trial config: {}\".format(best_trial.config))\n",
        "print(\"Best trial final validation loss: {}\".format(\n",
        "    best_trial.last_result[\"loss\"]))\n",
        "print(\"Best trial final validation accuracy: {}\".format(\n",
        "    best_trial.last_result[\"accuracy\"]))\n",
        "\n",
        "#tuner = tune.Tuner(\n",
        "#    tune.with_parameters(\n",
        "#        train, \n",
        "#        model=model, \n",
        "#        train_dataloader=train_dataloader, \n",
        "#        valid_dataloader=valid_dataloader),\n",
        "#    param_space=search_space)\n",
        "#results = tuner.fit()\n",
        "#print(results.get_best_result(metric=\"score\", mode=\"min\").config)\n",
        "\n",
        "#tuner = tune.Tuner(\n",
        "#    tune.with_resources(\n",
        "#          tune.with_parameters(train, model=model, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader),\n",
        "#    {\"gpus\": 1, \"cpus\": 1}),\n",
        "#    param_space=search_space)\n",
        "#results = tuner.fit()\n",
        "#print(results.get_best_result(metric=\"score\", mode=\"min\").config)\n",
        "\n",
        "\n",
        "#train(model, train_dataloader, valid_dataloader, config)"
      ],
      "metadata": {
        "id": "h3igVuFh_ZKK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96e0c5dd-9e1e-4994-c13e-93d2e642e4ac"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-01-23 07:08:28,439\tWARNING callback.py:108 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-23 07:08:28 (running for 00:00:00.14)\n",
            "Memory usage on this node: 7.4/12.7 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 1.000: None\n",
            "Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.7 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_2023-01-23_07-08-28\n",
            "Number of trials: 1/1 (1 RUNNING)\n",
            "+-------------------+----------+-------------------+---------+---------+-------+------------+-------------+----------------+\n",
            "| Trial name        | status   | loc               |   epoch |   gamma |    lr |   momentum |   step_size |   weight_decay |\n",
            "|-------------------+----------+-------------------+---------+---------+-------+------------+-------------+----------------|\n",
            "| train_bc88d_00000 | RUNNING  | 172.28.0.12:40276 |      20 |     0.1 | 0.005 |        1.8 |           6 |          0.001 |\n",
            "+-------------------+----------+-------------------+---------+---------+-------+------------+-------------+----------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m 2023-01-23 07:08:31,767\tERROR serialization.py:371 -- Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/_private/serialization.py\", line 369, in deserialize_objects\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/_private/serialization.py\", line 252, in _deserialize_object\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/_private/serialization.py\", line 207, in _deserialize_msgpack_data\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/_private/serialization.py\", line 197, in _deserialize_pickle5_data\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m     obj = pickle.loads(in_band)\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/torch/storage.py\", line 240, in _load_from_bytes\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m     return torch.load(io.BytesIO(b))\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 795, in load\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 1012, in _legacy_load\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m     result = unpickler.load()\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 958, in persistent_load\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m     wrap_storage=restore_location(obj, location),\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 215, in default_restore_location\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m     result = fn(storage, location)\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 182, in _cuda_deserialize\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m     device = validate_cuda_device(location)\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 166, in validate_cuda_device\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m     raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
            "\u001b[2m\u001b[36m(train pid=40276)\u001b[0m RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
            "2023-01-23 07:08:31,872\tERROR trial_runner.py:1088 -- Trial train_bc88d_00000: Error processing event.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/execution/ray_trial_executor.py\", line 1070, in get_next_executor_event\n",
            "    future_result = ray.get(ready_future)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/worker.py\", line 2309, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError: \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=40276, ip=172.28.0.12, repr=train)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/trainable/trainable.py\", line 367, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/trainable/function_trainable.py\", line 335, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/trainable/function_trainable.py\", line 652, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/trainable/util.py\", line 394, in _inner\n",
            "    return inner(config, checkpoint_dir=None)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/trainable/util.py\", line 385, in inner\n",
            "    fn_kwargs[k] = parameter_registry.get(prefix + k)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/registry.py\", line 234, in get\n",
            "    return ray.get(self.references[k])\n",
            "ray.exceptions.RaySystemError: System error: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
            "traceback: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/storage.py\", line 240, in _load_from_bytes\n",
            "    return torch.load(io.BytesIO(b))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 795, in load\n",
            "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 1012, in _legacy_load\n",
            "    result = unpickler.load()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 958, in persistent_load\n",
            "    wrap_storage=restore_location(obj, location),\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 215, in default_restore_location\n",
            "    result = fn(storage, location)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 182, in _cuda_deserialize\n",
            "    device = validate_cuda_device(location)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 166, in validate_cuda_device\n",
            "    raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
            "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"trialProgress\">\n",
              "  <h3>Trial Progress</h3>\n",
              "  <table>\n",
              "<thead>\n",
              "<tr><th>Trial name       </th><th>date               </th><th>experiment_id                   </th><th>hostname    </th><th>node_ip    </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  timestamp</th><th>trial_id   </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_bc88d_00000</td><td>2023-01-23_07-08-31</td><td>aa28586bc17b4985aa2683c011dda881</td><td>59882f235412</td><td>172.28.0.12</td><td style=\"text-align: right;\">40276</td><td style=\"text-align: right;\"> 1674457711</td><td>bc88d_00000</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>\n",
              "<style>\n",
              ".trialProgress {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".trialProgress h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".trialProgress td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-01-23 07:08:31 (running for 00:00:03.44)\n",
            "Memory usage on this node: 7.6/12.7 GiB \n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 1.000: None\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.41 GiB heap, 0.0/3.7 GiB objects (0.0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/train_2023-01-23_07-08-28\n",
            "Number of trials: 1/1 (1 ERROR)\n",
            "+-------------------+----------+-------------------+---------+---------+-------+------------+-------------+----------------+\n",
            "| Trial name        | status   | loc               |   epoch |   gamma |    lr |   momentum |   step_size |   weight_decay |\n",
            "|-------------------+----------+-------------------+---------+---------+-------+------------+-------------+----------------|\n",
            "| train_bc88d_00000 | ERROR    | 172.28.0.12:40276 |      20 |     0.1 | 0.005 |        1.8 |           6 |          0.001 |\n",
            "+-------------------+----------+-------------------+---------+---------+-------+------------+-------------+----------------+\n",
            "Number of errored trials: 1\n",
            "+-------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name        |   # failures | error file                                                                                                                                                                    |\n",
            "|-------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_bc88d_00000 |            1 | /root/ray_results/train_2023-01-23_07-08-28/train_bc88d_00000_0_epoch=20,gamma=0.1000,lr=0.0050,momentum=1.8000,step_size=6,weight_decay=0.0010_2023-01-23_07-08-28/error.txt |\n",
            "+-------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TuneError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-56b224062fa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m result = tune.run(\n\u001b[0m\u001b[1;32m     40\u001b[0m   \u001b[0;31m# Will throw ValueError: The actor ImplicitFunc is too large (158 MiB > FUNCTION_SIZE_ERROR_THRESHOLD=95 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;31m#partial(train, model, train_dataloader, valid_dataloader),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, _remote, _remote_string_queue)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"signal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [train_bc88d_00000])"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), SAVE_PATH + MODEL_NAME)"
      ],
      "metadata": {
        "id": "jSb8RYKH_Zug"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}